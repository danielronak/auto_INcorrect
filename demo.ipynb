{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **auto-INcorrect**\n",
        "The evil twin to your existing autocorrect models, It's a \"spell-checker-in-reverse\" that, instead of fixing mistakes, artfully creates them. This isn't just random chaos; it's a system that mimics exactly how humans mess up. From keyboard-adjacent \"fat-finger\" slips to those \"brain fart\" moments where you type \"your\" instead of \"you're.\" It's a way to prove we understand language so well, we can break it on purpose.\n",
        "\n",
        "Besides using this just for laughs, this is a great way to test autocorrect models. Using auto-INcorrect you can generate large amounts of \"wrong\" text with already available \"correct\" text and use it to train or test an autocorrect model. This idea is still in its infancy and there is is a lot of scope for creative modifications using deep learning systems. Personally, i have actually used this in some of my NLP projects and have found it to be quite useful."
      ],
      "metadata": {
        "id": "4IEk3esHx7UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The \"Fat Finger\" Module\n",
        "simulating keyboard errors. The foundation of this module is a dictionary that maps each letter to its neighbors on a standard QWERTY keyboard."
      ],
      "metadata": {
        "id": "cp1chBEPfuXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ivmJvhwIbmpU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# A dictionary mapping each key to its adjacent keys on a QWERTY keyboard\n",
        "KEY_NEIGHBORS = {\n",
        "    'q': ['w', 'a', 's','1'],\n",
        "    'w': ['q', 'e', 'a', 's', 'd','2'],\n",
        "    'e': ['w', 'r', 's', 'd', 'f','3'],\n",
        "    'r': ['e', 't', 'd', 'f', 'g','4'],\n",
        "    't': ['r', 'y', 'f', 'g', 'h','5'],\n",
        "    'y': ['t', 'u', 'g', 'h', 'j','6'],\n",
        "    'u': ['y', 'i', 'h', 'j', 'k','7'],\n",
        "    'i': ['u', 'o', 'j', 'k', 'l','8'],\n",
        "    'o': ['i', 'p', 'k', 'l','9'],\n",
        "    'p': ['o', 'l','0'],\n",
        "\n",
        "    'a': ['q', 'w', 's', 'z', 'x'],\n",
        "    's': ['q', 'w', 'e', 'a', 'd', 'z', 'x', 'c'],\n",
        "    'd': ['w', 'e', 'r', 's', 'f', 'x', 'c', 'v'],\n",
        "    'f': ['e', 'r', 't', 'd', 'g', 'c', 'v', 'b'],\n",
        "    'g': ['r', 't', 'y', 'f', 'h', 'v', 'b', 'n'],\n",
        "    'h': ['t', 'y', 'u', 'g', 'j', 'b', 'n', 'm'],\n",
        "    'j': ['y', 'u', 'i', 'h', 'k', 'n', 'm'],\n",
        "    'k': ['u', 'i', 'o', 'j', 'l', 'm'],\n",
        "    'l': ['i', 'o', 'p', 'k'],\n",
        "\n",
        "    'z': ['a', 's', 'x'],\n",
        "    'x': ['a', 's', 'd', 'z', 'c'],\n",
        "    'c': ['s', 'd', 'f', 'x', 'v'],\n",
        "    'v': ['d', 'f', 'g', 'c', 'b'],\n",
        "    'b': ['f', 'g', 'h', 'v', 'n'],\n",
        "    'n': ['g', 'h', 'j', 'b', 'm'],\n",
        "    'm': ['h', 'j', 'k', 'n'],\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adjacent_swap(word):\n",
        "  \"\"\"\n",
        "  Replaces a random character in a word with a keyboard-adjacent character.\n",
        "  \"\"\"\n",
        "  # Make sure the word is long enough and not just symbols\n",
        "  if len(word) < 1:\n",
        "    return word\n",
        "\n",
        "  # Pick a random index in the word\n",
        "  char_index = random.randint(0, len(word) - 1)\n",
        "  char_to_swap = word[char_index].lower() # Use lowercase for a clean dict lookup\n",
        "\n",
        "  # Check if the character is one we have neighbors for\n",
        "  if char_to_swap in KEY_NEIGHBORS:\n",
        "    # Pick a random neighbor\n",
        "    neighbor = random.choice(KEY_NEIGHBORS[char_to_swap])\n",
        "\n",
        "    # Rebuild the word: (part before) + (new char) + (part after)\n",
        "    # We'll preserve the original case of the neighbor\n",
        "    if word[char_index].isupper():\n",
        "        neighbor = neighbor.upper()\n",
        "\n",
        "    new_word = word[:char_index] + neighbor + word[char_index + 1:]\n",
        "    return new_word\n",
        "\n",
        "  # If the character wasn't in our map (like '!' or ','), just return the original word\n",
        "  return word"
      ],
      "metadata": {
        "id": "fAxNcecEchcw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "QbULhIe0dF5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing 'standard':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {adjacent_swap('standard')}\")\n",
        "\n",
        "print(\"\\nTesting 'example':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {adjacent_swap('example')}\")\n",
        "\n",
        "print(\"\\nTesting 'Python':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {adjacent_swap('Python')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHrzwGVqc5PQ",
        "outputId": "1ad1439a-bbeb-4ae7-8779-4e6be51aba70",
        "collapsed": true
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 'standard':\n",
            "  -> stqndard\n",
            "  -> standare\n",
            "  -> standqrd\n",
            "  -> stqndard\n",
            "  -> standafd\n",
            "\n",
            "Testing 'example':\n",
            "  -> rxample\n",
            "  -> examole\n",
            "  -> dxample\n",
            "  -> exampke\n",
            "  -> exqmple\n",
            "\n",
            "Testing 'Python':\n",
            "  -> Pgthon\n",
            "  -> Pytbon\n",
            "  -> Pythln\n",
            "  -> Pythom\n",
            "  -> Pythpn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transpose(word):\n",
        "  \"\"\"\n",
        "  Swaps two random adjacent characters in a word.\n",
        "  \"\"\"\n",
        "  # Can't transpose if the word is 1 or 0 chars long\n",
        "  if len(word) < 2:\n",
        "    return word\n",
        "\n",
        "  # Pick a random index to be the *first* char of the swap\n",
        "  # We -2 because we need to be able to select its neighbor (index + 1)\n",
        "  # So for \"word\" (len 4), we can pick index 0 ('w'), 1 ('o'), or 2 ('r')\n",
        "  char_index = random.randint(0, len(word) - 2)\n",
        "\n",
        "  # Rebuild the word with the two chars swapped\n",
        "  new_word = (\n",
        "      word[:char_index] +         # Part before the swap\n",
        "      word[char_index + 1] +    # The second char\n",
        "      word[char_index] +        # The first char\n",
        "      word[char_index + 2:]     # The part after\n",
        "  )\n",
        "\n",
        "  return new_word"
      ],
      "metadata": {
        "id": "oKLYIwPMc6A0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "2KrwEOUadXzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing 'standard':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {transpose('standard')}\")\n",
        "\n",
        "print(\"\\nTesting 'example':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {transpose('example')}\")\n",
        "\n",
        "print(\"\\nTesting 'Python':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {transpose('Python')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JI41pVKqdRL8",
        "outputId": "582a0d3d-a8ba-4868-cc79-a620a343cf33"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 'standard':\n",
            "  -> stnadard\n",
            "  -> stnadard\n",
            "  -> standrad\n",
            "  -> tsandard\n",
            "  -> stanadrd\n",
            "\n",
            "Testing 'example':\n",
            "  -> xeample\n",
            "  -> examlpe\n",
            "  -> examlpe\n",
            "  -> xeample\n",
            "  -> xeample\n",
            "\n",
            "Testing 'Python':\n",
            "  -> Pytohn\n",
            "  -> Pythno\n",
            "  -> Pythno\n",
            "  -> Pyhton\n",
            "  -> yPthon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_char(word):\n",
        "  \"\"\"\n",
        "  Deletes a random character from a word.\n",
        "  \"\"\"\n",
        "  # Can't delete if the word is 1 char or less\n",
        "  if len(word) < 2:\n",
        "    return word\n",
        "\n",
        "  # Pick a random index to delete\n",
        "  char_index = random.randint(0, len(word) - 1)\n",
        "\n",
        "  # Rebuild the word without that character\n",
        "  new_word = word[:char_index] + word[char_index + 1:]\n",
        "\n",
        "  return new_word"
      ],
      "metadata": {
        "id": "FwddGHczdaCr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "rIhC7rFhdt7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing 'standard':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {delete_char('standard')}\")\n",
        "\n",
        "print(\"\\nTesting 'example':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {delete_char('example')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Anq-pxrkdqbd",
        "outputId": "313ba726-fd31-4cdc-8669-22ee58f71142"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 'standard':\n",
            "  -> stadard\n",
            "  -> standad\n",
            "  -> stndard\n",
            "  -> sandard\n",
            "  -> tandard\n",
            "\n",
            "Testing 'example':\n",
            "  -> exaple\n",
            "  -> exaple\n",
            "  -> exaple\n",
            "  -> exampe\n",
            "  -> exmple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_char(word):\n",
        "  \"\"\"\n",
        "  Inserts a random keyboard-adjacent character into a word.\n",
        "  \"\"\"\n",
        "  if len(word) < 1:\n",
        "    return word\n",
        "\n",
        "  # Pick a random index *in* the word to insert *next to*\n",
        "  # (Including the end of the word)\n",
        "  insert_index = random.randint(0, len(word) - 1)\n",
        "  char_to_neighbor = word[insert_index].lower()\n",
        "\n",
        "  # Find a neighbor to insert\n",
        "  if char_to_neighbor in KEY_NEIGHBORS:\n",
        "    neighbor = random.choice(KEY_NEIGHBORS[char_to_neighbor])\n",
        "\n",
        "    # Preserve case if the original char was uppercase\n",
        "    if word[insert_index].isupper():\n",
        "        neighbor = neighbor.upper()\n",
        "\n",
        "    # Rebuild the word\n",
        "    # We'll insert *after* the chosen character\n",
        "    new_word = word[:insert_index + 1] + neighbor + word[insert_index + 1:]\n",
        "    return new_word\n",
        "\n",
        "  # If we can't find a neighbor (e.g., it's a symbol), just return the word\n",
        "  return word"
      ],
      "metadata": {
        "id": "HzTk2Ga3dvOL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "irXMwYBud6XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing 'standard':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {insert_char('standard')}\")\n",
        "\n",
        "print(\"\\nTesting 'example':\")\n",
        "for _ in range(5):\n",
        "  print(f\"  -> {insert_char('example')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-aqm8BFzd3IY",
        "outputId": "074125a5-9cc4-4145-bbf8-0d7a6ad6203f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 'standard':\n",
            "  -> standfard\n",
            "  -> stazndard\n",
            "  -> stanmdard\n",
            "  -> sctandard\n",
            "  -> staqndard\n",
            "\n",
            "Testing 'example':\n",
            "  -> example3\n",
            "  -> efxample\n",
            "  -> excample\n",
            "  -> exampole\n",
            "  -> examplef\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fat_finger_module(word):\n",
        "  \"\"\"\n",
        "  Selects one of the four typographical error functions based on\n",
        "  a weighted probability and applies it to the given word.\n",
        "  \"\"\"\n",
        "  # List of all the error functions we've built\n",
        "  error_functions = [\n",
        "      adjacent_swap,\n",
        "      transpose,\n",
        "      delete_char,\n",
        "      insert_char\n",
        "  ]\n",
        "\n",
        "  # [swap, transpose, delete, insert]\n",
        "  weights = [0.4, 0.3, 0.2, 0.1]\n",
        "\n",
        "  # random.choices returns a list, so we get the first (and only) item [0]\n",
        "  chosen_function = random.choices(error_functions, weights=weights, k=1)[0]\n",
        "\n",
        "  # Call the chosen function with the word\n",
        "  return chosen_function(word)"
      ],
      "metadata": {
        "id": "FBJ9Xhtvd8tk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "obU4lJYJfMDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running the 'Fat Finger' module on 'standard':\")\n",
        "for _ in range(20):\n",
        "  print(f\"  -> {run_fat_finger_module('standard')}\")\n",
        "\n",
        "print(\"\\nRunning the 'Fat Finger' module on 'Python':\")\n",
        "for _ in range(20):\n",
        "  print(f\"  -> {run_fat_finger_module('Python')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xciOp4pifGp-",
        "outputId": "e7d3d757-2e89-4912-eee5-898e78006810"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the 'Fat Finger' module on 'standard':\n",
            "  -> standarx\n",
            "  -> stanfard\n",
            "  -> standadr\n",
            "  -> stawndard\n",
            "  -> sfandard\n",
            "  -> stndard\n",
            "  -> stanhdard\n",
            "  -> standaxrd\n",
            "  -> standadr\n",
            "  -> standadr\n",
            "  -> stndard\n",
            "  -> ztandard\n",
            "  -> stndard\n",
            "  -> standaed\n",
            "  -> standrad\n",
            "  -> atandard\n",
            "  -> standeard\n",
            "  -> standarx\n",
            "  -> standardw\n",
            "  -> standadd\n",
            "\n",
            "Running the 'Fat Finger' module on 'Python':\n",
            "  -> ython\n",
            "  -> Pthon\n",
            "  -> Pythn\n",
            "  -> Pyhton\n",
            "  -> Pythonh\n",
            "  -> Pytbon\n",
            "  -> Pythno\n",
            "  -> Pytohn\n",
            "  -> Ptyhon\n",
            "  -> Pytbon\n",
            "  -> Pythln\n",
            "  -> Pyhton\n",
            "  -> Ptyhon\n",
            "  -> Pythoj\n",
            "  -> Pytghon\n",
            "  -> Pyhhon\n",
            "  -> Pythno\n",
            "  -> 0ython\n",
            "  -> Pythkn\n",
            "  -> P6thon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The \"Brain Fart\" Module\n",
        "This module simulates cognitive mistakes, where the typing is correct but the word choice is wrong. We'll start with the most common one: homophone replacement.\n",
        "\n",
        "This is when someone uses \"their\" instead of \"there,\" or \"it's\" instead of \"its.\""
      ],
      "metadata": {
        "id": "qE6072ZNgFuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionary mapping common homophones.\n",
        "# We'll store them in sets for easy lookup and manipulation.\n",
        "HOMOPHONES = {\n",
        "    'group_1': {'their', 'there', 'they\\'re'},\n",
        "    'group_2': {'your', 'you\\'re'},\n",
        "    'group_3': {'its', 'it\\'s'},\n",
        "    'group_4': {'to', 'too', 'two'},\n",
        "    'group_5': {'affect', 'effect'},\n",
        "    'group_6': {'weather', 'whether'},\n",
        "    'group_7': {'peace', 'piece'},\n",
        "    'group_8': {'break', 'brake'},\n",
        "    'group_9': {'buy', 'by', 'bye'},\n",
        "}\n",
        "\n",
        "# We also need a fast way to look up *any* of these words\n",
        "# This \"flattens\" the dictionary for quick checks.\n",
        "HOMOPHONE_MAP = {}\n",
        "for group_set in HOMOPHONES.values():\n",
        "    for word in group_set:\n",
        "        # We store the *other* words in the set as the replacement options\n",
        "        replacements = group_set - {word}\n",
        "        HOMOPHONE_MAP[word] = list(replacements)\n",
        "\n",
        "\n",
        "def replace_homophone(word):\n",
        "  \"\"\"\n",
        "  Replaces a word with a random homophone, if one is found.\n",
        "  Preserves the original capitalization.\n",
        "  \"\"\"\n",
        "  word_lower = word.lower()\n",
        "\n",
        "  if word_lower in HOMOPHONE_MAP:\n",
        "    # Get the list of possible replacements\n",
        "    replacements = HOMOPHONE_MAP[word_lower]\n",
        "\n",
        "    # Pick one random replacement\n",
        "    new_word = random.choice(replacements)\n",
        "\n",
        "    # Preserve capitalization\n",
        "    if word.isupper():\n",
        "      return new_word.upper()\n",
        "    elif word.istitle(): # Capitalizes the first letter\n",
        "      return new_word.capitalize()\n",
        "    else:\n",
        "      return new_word\n",
        "\n",
        "  # If no homophone was found, return the original word\n",
        "  return word"
      ],
      "metadata": {
        "id": "EhpsiRSdfOUH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "kB6zxqzVh0oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"'their'  -> {replace_homophone('their')}\")\n",
        "print(f\"'There'  -> {replace_homophone('There')}\")\n",
        "print(f\"'YOU'RE'  -> {replace_homophone('YOU\\'RE')}\")\n",
        "print(f\"'Its'    -> {replace_homophone('Its')}\")\n",
        "print(f\"'to'     -> {replace_homophone('to')}\")\n",
        "print(f\"'hello'  -> {replace_homophone('hello')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae_cEVvwhjiU",
        "outputId": "a8ff1fd6-5fe3-4ddf-a707-62dc0f6d74ac"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'their'  -> they're\n",
            "'There'  -> They're\n",
            "'YOU'RE'  -> YOUR\n",
            "'Its'    -> It's\n",
            "'to'     -> two\n",
            "'hello'  -> hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvx0hqUYh2zI",
        "outputId": "5be6ccde-e941-4b22-f99c-60cf09274808"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function replaces a word with a related word, accepts a pos_tag to find the right lemma."
      ],
      "metadata": {
        "id": "Jgtv6Fo6RKLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Helper function to convert NLTK's POS tag to a WordNet-compatible tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Converts NLTK's POS tag to a tag WordNet understands.\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN # Default to noun\n",
        "\n",
        "def replace_morphological(word, pos_tag):\n",
        "    \"\"\"\n",
        "    Replaces a word with a related word.\n",
        "    accepts a pos_tag to find the right lemma.\n",
        "    \"\"\"\n",
        "    # 1. Convert the tag\n",
        "    wordnet_pos = get_wordnet_pos(pos_tag)\n",
        "\n",
        "    # 2. Find all \"synsets\"\n",
        "    synsets = wordnet.synsets(word, pos=wordnet_pos)\n",
        "    if not synsets:\n",
        "        return word\n",
        "\n",
        "    # 3. Get all related lemma names\n",
        "    related_lemmas = set()\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            related_lemmas.add(lemma.name())\n",
        "\n",
        "    # 4. Filter\n",
        "    original_lower = word.lower()\n",
        "    replacements = [\n",
        "        lemma for lemma in related_lemmas\n",
        "        if lemma.lower() != original_lower and '_' not in lemma\n",
        "    ]\n",
        "\n",
        "    if replacements:\n",
        "        # 5. Pick a replacement and preserve case\n",
        "        new_word = random.choice(replacements)\n",
        "        if word.isupper():\n",
        "            return new_word.upper()\n",
        "        elif word.istitle():\n",
        "            return new_word.capitalize()\n",
        "        else:\n",
        "            return new_word\n",
        "\n",
        "    return word"
      ],
      "metadata": {
        "id": "I_9Wj8-_iRw-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_brain_fart_module(word, pos_tag):\n",
        "    \"\"\"\n",
        "    Selects one of the two cognitive error functions.\n",
        "    \"\"\"\n",
        "    error_functions = [\n",
        "        replace_homophone,\n",
        "        replace_morphological\n",
        "    ]\n",
        "    weights = [0.7, 0.3]\n",
        "\n",
        "    chosen_function = random.choices(error_functions, weights=weights, k=1)[0]\n",
        "\n",
        "    # Pass the tag to the \"smart\" function if it's chosen\n",
        "    if chosen_function == replace_morphological:\n",
        "        return chosen_function(word, pos_tag) # Pass the tag\n",
        "    else:\n",
        "        return chosen_function(word) # Homophone doesn't need it"
      ],
      "metadata": {
        "id": "hBJ4YS8ikSDq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the \"Malapropism\" Module\n",
        "The \"Malapropism\" module finds the root of a word (like 'banking' -> 'bank') and looks up semantically similar words in a machine learning model. It then replaces the original with a word that is both semantically related *and* shares the same root (like 'banker'), creating a plausible \"near-miss\" error."
      ],
      "metadata": {
        "id": "Lij0kSXwNy8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMcEd6JUljhH",
        "outputId": "13e2dea3-3d2c-4cc3-b107-c984e50f6365"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "\n",
        "print(\"Loading GloVe\")\n",
        "try:\n",
        "    glove_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# 'most_similar' is the k-NN function we'll be using.\n",
        "if 'glove_model' in globals():\n",
        "    try:\n",
        "        print(\"\\nTesting model with 'king':\")\n",
        "        print(glove_model.most_similar('king'))\n",
        "    except KeyError:\n",
        "        print(\"\\n'king' not in vocabulary, but model is loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMRUT9Ki4iJE",
        "outputId": "664e327d-5540-432e-e9c3-17dd9b60b3e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FloVe\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Model loaded successfully.\n",
            "\n",
            "Testing model with 'king':\n",
            "[('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175), ('throne', 0.6919989585876465), ('kingdom', 0.6811409592628479), ('father', 0.6802029013633728), ('emperor', 0.6712858080863953), ('ii', 0.6676074266433716)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# NEW, MODIFIED FUNCTION\n",
        "def replace_semantic_neighbor(word, pos_tag):\n",
        "    \"\"\"\n",
        "    Replaces a word with a semantically similar, root-sharing word.\n",
        "    NOW ACCEPTS a pos_tag to find the right lemma.\n",
        "    \"\"\"\n",
        "    word_lower = word.lower()\n",
        "\n",
        "    # 1. Convert tag and get the *correct* lemma. No more guessing!\n",
        "    wordnet_pos = get_wordnet_pos(pos_tag)\n",
        "    lemma = lemmatizer.lemmatize(word_lower, pos=wordnet_pos)\n",
        "\n",
        "    # 2. Check if the lemma is in our model\n",
        "    if lemma not in glove_model:\n",
        "        return word\n",
        "\n",
        "    try:\n",
        "        # 3. Get neighbors of the LEMMA\n",
        "        similar_words = glove_model.most_similar(lemma, topn=10)\n",
        "\n",
        "        # 4. Filter\n",
        "        replacements = []\n",
        "        for w_tuple in similar_words:\n",
        "            w = w_tuple[0].lower() # Neighbor word\n",
        "            if w.startswith(lemma) and w != word_lower:\n",
        "                replacements.append(w)\n",
        "\n",
        "        if replacements:\n",
        "            # 5. Pick one and preserve case\n",
        "            new_word = random.choice(replacements)\n",
        "            if word.isupper():\n",
        "                return new_word.upper()\n",
        "            elif word.istitle():\n",
        "                return new_word.capitalize()\n",
        "            else:\n",
        "                return new_word\n",
        "\n",
        "        return word\n",
        "\n",
        "    except Exception as e:\n",
        "        return word"
      ],
      "metadata": {
        "id": "4DheAgdP9jZr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_malapropism_module(word, pos_tag): # Added pos_tag\n",
        "    \"\"\"\n",
        "    Selects one of the semantic error functions.\n",
        "    \"\"\"\n",
        "    # Just call our final function and pass the tag\n",
        "    return replace_semantic_neighbor(word, pos_tag)"
      ],
      "metadata": {
        "id": "ZbrVWeFh-4E6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkg_e7B1CV7b",
        "outputId": "ce81a5e6-d1ca-4dcd-e3dd-b000f742d833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Punctuation Corruption\" Module\n",
        "\n",
        "This module is a bit different because it needs to handle two separate jobs:\n",
        "\n",
        "Corrupting existing punctuation (remove/replace).\n",
        "\n",
        "Adding new, wrong punctuation to words."
      ],
      "metadata": {
        "id": "brAJZzS3GmLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import random\n",
        "\n",
        "# --- Define our punctuation sets ---\n",
        "PUNCTUATION_MARKS = {'.', ',', '!', '?', ';', ':'}\n",
        "REPLACEMENT_PUNCTUATION = [',', '!', '?', ';']\n",
        "\n",
        "def corrupt_punctuation(text,\n",
        "                   add_rate=0.1,\n",
        "                   replace_rate=0.3,\n",
        "                   remove_rate=0.6,\n",
        "                   case_corruption_prob=0.5): # <-- Added this parameter\n",
        "    \"\"\"\n",
        "    A standalone function to corrupt *both* punctuation AND case in a text.\n",
        "\n",
        "    :param text: The input string.\n",
        "    :param add_rate: Probability of adding punctuation *after* any given word.\n",
        "    :param replace_rate: Probability of replacing *existing* punctuation.\n",
        "    :param remove_rate: Probability of removing *existing* punctuation.\n",
        "    :param case_corruption_prob: Probability of flipping a capital letter to lowercase.\n",
        "    \"\"\"\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    corrupted_words = []\n",
        "\n",
        "    for word in words:\n",
        "\n",
        "        # --- 1. Logic for tokens that ARE punctuation (e.g., '.') ---\n",
        "        if word in PUNCTUATION_MARKS:\n",
        "            # First, check if we remove it\n",
        "            if random.random() < remove_rate:\n",
        "                continue # Skip appending it\n",
        "\n",
        "            # Second, check if we replace it\n",
        "            elif random.random() < replace_rate:\n",
        "                new_punct = random.choice(REPLACEMENT_PUNCTUATION)\n",
        "                while new_punct == word: # Avoid replacing '?' with '?'\n",
        "                    new_punct = random.choice(REPLACEMENT_PUNCTUATION)\n",
        "                corrupted_words.append(new_punct)\n",
        "\n",
        "            # If neither, append the original\n",
        "            else:\n",
        "                corrupted_words.append(word)\n",
        "\n",
        "        # --- 2. Logic for tokens that are WORDS ---\n",
        "        else:\n",
        "\n",
        "            # --- START: Added Case Corruption Logic ---\n",
        "            # Loop through the word and apply random case flips\n",
        "            new_chars = [\n",
        "                char.lower() if char.isupper() and random.random() < case_corruption_prob else char\n",
        "                for char in word\n",
        "            ]\n",
        "            corrupted_word = \"\".join(new_chars)\n",
        "            # --- END: Added Case Corruption Logic ---\n",
        "\n",
        "\n",
        "            # First, append the (now possibly case-corrupted) word\n",
        "            corrupted_words.append(corrupted_word)\n",
        "\n",
        "            # Second, check if we should ADD punctuation after it\n",
        "            if random.random() < add_rate:\n",
        "                corrupted_words.append(random.choice(REPLACEMENT_PUNCTUATION))\n",
        "\n",
        "    # Re-assemble the sentence\n",
        "    detokenizer = TreebankWordDetokenizer()\n",
        "    final_text = detokenizer.detokenize(corrupted_words)\n",
        "\n",
        "    return final_text"
      ],
      "metadata": {
        "id": "RuIgbgBRIh0K"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final 'word-error' layer"
      ],
      "metadata": {
        "id": "vRWF8Kvxudmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import random\n",
        "\n",
        "# --- This is our default \"mix\" of errors if the user doesn't specify one ---\n",
        "DEFAULT_DISTRIBUTION = {\n",
        "    'fat_finger': 0.4,   # Back to 60%\n",
        "    'brain_fart': 0.3,   # Back to 30%\n",
        "    'malapropism': 0.3   # Back to 10%\n",
        "}\n",
        "\n",
        "# We have REMOVED 'punctuation_error_rate' and all punctuation logic.\n",
        "def word_error(text, error_rate=0.15, distribution=None):\n",
        "    \"\"\"\n",
        "    Intentionally introduces human-like errors into a text string.\n",
        "    THIS VERSION ONLY AFFECTS WORDS, NOT PUNCTUATION.\n",
        "\n",
        "    :param text: The input string.\n",
        "    :param error_rate: A float (0.0 to 1.0) for the probability\n",
        "                       that any given word will be corrupted.\n",
        "    :param distribution: A dict specifying the weights for each\n",
        "                         error module. e.g., {'fat_finger': 0.7, ...}\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Set the distribution\n",
        "    if distribution is None:\n",
        "        distribution = DEFAULT_DISTRIBUTION\n",
        "\n",
        "    module_names = list(distribution.keys())\n",
        "    module_weights = list(distribution.values())\n",
        "\n",
        "    # 2. Tokenize and get POS tags\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "\n",
        "    corrupted_words = []\n",
        "\n",
        "    # 3. Loop through each (word, tag)\n",
        "    for word, tag in tagged_words:\n",
        "\n",
        "        # 4. Check if it's punctuation. If so, skip it.\n",
        "        # We'll use a basic check.\n",
        "        if not word.isalnum(): # isalnum() is False if the token is '.', ',', '!', etc.\n",
        "            corrupted_words.append(word)\n",
        "            continue # Go to the next token\n",
        "\n",
        "        # 5. Decide *if* we should apply an error (to this word)\n",
        "        if random.random() < error_rate:\n",
        "\n",
        "            # 6. Decide *which* module to use\n",
        "            chosen_module = random.choices(module_names, weights=module_weights, k=1)[0]\n",
        "\n",
        "            # 7. Call the chosen module\n",
        "            if chosen_module == 'fat_finger':\n",
        "                corrupted_word = run_fat_finger_module(word)\n",
        "            elif chosen_module == 'brain_fart':\n",
        "                corrupted_word = run_brain_fart_module(word, tag)\n",
        "            elif chosen_module == 'malapropism':\n",
        "                corrupted_word = run_malapropism_module(word, tag)\n",
        "            else:\n",
        "                corrupted_word = word # Failsafe\n",
        "\n",
        "            corrupted_words.append(corrupted_word)\n",
        "\n",
        "        else:\n",
        "            # No error applied, just add the original word\n",
        "            corrupted_words.append(word)\n",
        "\n",
        "    # 8. Re-assemble the sentence\n",
        "    detokenizer = TreebankWordDetokenizer()\n",
        "    final_text = detokenizer.detokenize(corrupted_words)\n",
        "\n",
        "    return final_text"
      ],
      "metadata": {
        "id": "cX7Hv0-8CtrQ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "DoVnwxK9un__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = \"Wardell Stephen Curry II, also known as Steph Curry, is an American professional basketball player for the Golden State Warriors of the National Basketball Association, where he plays as a point guard.\"\n",
        "\n",
        "print(f\"ORIGINAL:\\n{original_text}\\n\")\n",
        "\n",
        "# --- Test 1: auto_incorrect ONLY (Word errors) ---\n",
        "print(\"--- 1. WORD ERRORS ONLY ---\")\n",
        "corrupted_words = word_error(original_text, error_rate=0.3)\n",
        "print(f\"  -> {corrupted_words}\\n\")\n",
        "\n",
        "\n",
        "# --- Test 2: corrupt_punctuation ONLY (Punctuation errors) ---\n",
        "print(\"--- 2. PUNCTUATION ERRORS ONLY ---\")\n",
        "corrupted_punct = corrupt_punctuation(original_text, add_rate=0.2, remove_rate=0.5, replace_rate=0.5, case_corruption_prob=0.5)\n",
        "print(f\"  -> {corrupted_punct}\\n\")\n",
        "\n",
        "\n",
        "# --- Test 3: Chaining BOTH functions ---\n",
        "print(\"--- 3. CHAINING BOTH (Word errors THEN Punctuation errors) ---\")\n",
        "# First, apply word errors\n",
        "output_1 = word_error(original_text, error_rate=0.3)\n",
        "# Then, apply punctuation errors to that output\n",
        "output_2 = corrupt_punctuation(output_1, add_rate=0.2, remove_rate=0.5, replace_rate=0.5)\n",
        "print(f\"  -> {output_2}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_-UH6nnIkbM",
        "outputId": "bc17cf83-293a-4c7f-d252-b5c4bf44c774",
        "collapsed": true
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL:\n",
            "Wardell Stephen Curry II, also known as Steph Curry, is an American professional basketball player for the Golden State Warriors of the National Basketball Association, where he plays as a point guard.\n",
            "\n",
            "--- 1. WORD ERRORS ONLY ---\n",
            "  -> Wardell Stephen Curry II, also known as Steph Curry, being na American professional basketball player for the Goldeg State Warriors of the National Basketball Association, where he player s a point guard.\n",
            "\n",
            "--- 2. PUNCTUATION ERRORS ONLY ---\n",
            "  -> wardell Stephen Curry iI also known! as steph Curry, is an; American professional basketball player for the Golden state Warriors of the National Basketball Association where he plays as a point guard,.\n",
            "\n",
            "--- 3. CHAINING BOTH (Word errors THEN Punctuation errors) ---\n",
            "  -> Wardell! stephen curry? ii! also, jnown as! steph? Curry, is an American professional, basketball player for the golden state! wsrriors? of fhe; subject basketball Association where he plays an a point? guard\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto_incorrect, the final function"
      ],
      "metadata": {
        "id": "wbGdsBttuto-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_incorrect(input_text,\n",
        "                        # Parameters for word_error\n",
        "                        error_rate=0.15,\n",
        "                        distribution=None,\n",
        "\n",
        "                        # Parameters for corrupt_punctuation\n",
        "                        add_rate=0.1,\n",
        "                        replace_rate=0.3,\n",
        "                        remove_rate=0.3,\n",
        "                        case_corruption_prob=0.5):\n",
        "  \"\"\"\n",
        "  Runs the input text through the full corruption chain.\n",
        "  Allows tweaking all parameters or using the defaults.\n",
        "\n",
        "  :param input_text: The string to corrupt.\n",
        "  :param error_rate: (from word_error) Probability of corrupting a word.\n",
        "  :param distribution: (from word_error) Dict of word error weights.\n",
        "  :param add_rate: (from corrupt_punctuation) Probability of adding punctuation.\n",
        "  :param replace_rate: (from corrupt_punctuation) Probability of replacing punctuation.\n",
        "  :param remove_rate: (from corrupt_punctuation) Probability of removing punctuation.\n",
        "  \"\"\"\n",
        "\n",
        "  # Step 1: Apply word errors, passing the user's (or default) settings\n",
        "  text_with_word_errors = word_error( # Changed from auto_incorrect to word_error\n",
        "      input_text,\n",
        "      error_rate=error_rate,\n",
        "      distribution=distribution\n",
        "  )\n",
        "\n",
        "  # Step 2: Apply punctuation errors, passing the user's (or default) settings\n",
        "  final_corrupted_text = corrupt_punctuation(\n",
        "      text_with_word_errors,\n",
        "      add_rate=add_rate,\n",
        "      replace_rate=replace_rate,\n",
        "      remove_rate=remove_rate,\n",
        "      case_corruption_prob=case_corruption_prob\n",
        "  )\n",
        "\n",
        "  return final_corrupted_text\n",
        "\n",
        "# --- How to use it ---\n",
        "\n",
        "my_text = \"I am running a test on my banking application for its creative standards. It's a beautiful day to see if your function works correctly.\"\n",
        "\n",
        "print(f\"Original Text:\\n{my_text}\\n\")\n",
        "\n",
        "# --- Example 1: Calling with all default settings ---\n",
        "# (This will work just like the last function)\n",
        "default_output = auto_incorrect(my_text)\n",
        "print(f\"Fully Corrupted (Default Settings):\\n{default_output}\\n\")\n",
        "\n",
        "# --- Example 2: Calling with custom, tweaked settings ---\n",
        "# Let's make it very aggressive and only use 'fat_finger' typos\n",
        "# and add a lot of punctuation.\n",
        "\n",
        "# 1. Define a custom distribution\n",
        "custom_distribution = {\n",
        "    'fat_finger': 1.0,\n",
        "    'brain_fart': 0.0,\n",
        "    'malapropism': 0.0\n",
        "}\n",
        "\n",
        "# 2. Call the function with new values\n",
        "tweaked_output = auto_incorrect(\n",
        "    my_text,\n",
        "    error_rate=0.3,\n",
        "    distribution=custom_distribution,\n",
        "    add_rate=0.1,\n",
        "    remove_rate=0.6,\n",
        "    replace_rate=0.3,\n",
        "    case_corruption_prob=0.5\n",
        ")\n",
        "\n",
        "print(f\"Fully Corrupted (Aggressive Tweaked Settings):\\n{tweaked_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6Zz3WiKFET",
        "outputId": "d8b718ae-e500-4cee-9e56-bedda305b912"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "I am running a test on my banking application for its creative standards. It's a beautiful day to see if your function works correctly.\n",
            "\n",
            "Fully Corrupted (Default Settings):\n",
            "i m running a test on my banking application for its creative standards It's! a beautiful day to see if your function works correctly;\n",
            "\n",
            "Fully Corrupted (Aggressive Tweaked Settings):\n",
            "I am running a test on! mu banknig applicaion for its creativ satndards, it's a baeutiful dya to sef! if your function; wkrks correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae1726e",
        "outputId": "d3bb85f5-98f7-4e64-a053-3514d261c337"
      },
      "source": [
        "custom_text = input(\"enter the text to be corrupted: \")\n",
        "\n",
        "custom_distribution = {\n",
        "    'fat_finger': 0.5,\n",
        "    'brain_fart': 0.3,\n",
        "    'malapropism': 0.2\n",
        "}\n",
        "\n",
        "corrupted_custom_text = auto_incorrect(custom_text,error_rate= 0.0, distribution= custom_distribution, add_rate=0.1, remove_rate=0.8, replace_rate=0.3, case_corruption_prob=0.3)\n",
        "#error_rate is the\n",
        "print(f\"Original Text:\\n{custom_text}\\n\")\n",
        "print(f\"Corrupted Text:\\n{corrupted_custom_text}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the text to be corrupted: The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary; rather, it's a dynamic entity deeply intertwined with context, culture, and subtle intention. Natural Language Processing (NLP) models, often built on vast statistical analysis, strive to comprehend and generate text with human-like fluency. However, a fascinating inverse problem exists: modeling human error. Simulating a typo isn't just random substitution; it involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems. \n",
            "Original Text:\n",
            "The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary; rather, it's a dynamic entity deeply intertwined with context, culture, and subtle intention. Natural Language Processing (NLP) models, often built on vast statistical analysis, strive to comprehend and generate text with human-like fluency. However, a fascinating inverse problem exists: modeling human error. Simulating a typo isn't just random substitution; it involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems. \n",
            "\n",
            "Corrupted Text:\n",
            "the? complexity of human language presents a profound challenge for artificial intelligence, it's not merely a structured system of vocabulary rather, it's a dynamic entity deeply intertwined with context culture and, subtle intention natural language Processing (NLP) models often; built on vast statistical analysis strive to comprehend and generate text with? human-like fluency? However a fascinating inverse problem exists! modeling human error Simulating a typo isn't just random? substitution it involves understanding keyboard layouts common transpositions and phonetic similarities More advanced simulations such, as semantic errors require a sophisticated grasp of how the human mind retrieves and associates words This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method! for building and evaluating more robust correction? systems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use in testing"
      ],
      "metadata": {
        "id": "fDol7VvM40NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "rnW20o5-MlDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dd3daf-a9d0-4a64-cd2d-71c400912d65"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "def check_similarity(str1, str2):\n",
        "  \"\"\"\n",
        "  Checks the similarity between two strings using fuzzywuzzy.\n",
        "\n",
        "  Returns:\n",
        "    An integer (0-100) representing the similarity score.\n",
        "  \"\"\"\n",
        "\n",
        "  # fuzz.ratio() calculates the Levenshtein distance similarity\n",
        "  similarity_score = fuzz.ratio(str1, str2)\n",
        "  return similarity_score\n",
        "\n",
        "str1 = input(\"enter the correct string\" )\n",
        "str2 = input(\"enter the correcTED string\" )\n",
        "\n",
        "score = check_similarity(str1, str2)\n",
        "\n",
        "\n",
        "print(f\"Str1: {str1}\")\n",
        "print(f\"Str2: {str2}\")\n",
        "\n",
        "print(f\"Similarity Score: {score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSuWenCMl8zv",
        "outputId": "5b9aeee2-35f9-4834-8552-34979f260079"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the correct stringThe complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary; rather, it's a dynamic entity deeply intertwined with context, culture, and subtle intention. Natural Language Processing (NLP) models, often built on vast statistical analysis, strive to comprehend and generate text with human-like fluency. However, a fascinating inverse problem exists: modeling human error. Simulating a typo isn't just random substitution; it involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems. \n",
            "enter the correcTED stringThe complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary, rather it's a dynamic entity deeply intertwined with context culture and subtle intention. Natural language processing ( NLP ) models, often built on vast statistical analysis, strive to comprehend and generate text with humanlike fluency. However, a fascinating inverse problem exists, modeling human error. Simulating a typo isn't just random substitution it. Involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This reverse engineering of mistakes purposefully creating plausible incorrectness is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.\n",
            "Str1: The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary; rather, it's a dynamic entity deeply intertwined with context, culture, and subtle intention. Natural Language Processing (NLP) models, often built on vast statistical analysis, strive to comprehend and generate text with human-like fluency. However, a fascinating inverse problem exists: modeling human error. Simulating a typo isn't just random substitution; it involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This \"reverse engineering\" of mistakes—purposefully creating plausible incorrectness—is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems. \n",
            "Str2: The complexity of human language presents a profound challenge for artificial intelligence. It's not merely a structured system of vocabulary, rather it's a dynamic entity deeply intertwined with context culture and subtle intention. Natural language processing ( NLP ) models, often built on vast statistical analysis, strive to comprehend and generate text with humanlike fluency. However, a fascinating inverse problem exists, modeling human error. Simulating a typo isn't just random substitution it. Involves understanding keyboard layouts, common transpositions, and phonetic similarities. More advanced simulations, such as semantic errors, require a sophisticated grasp of how the human mind retrieves and associates words. This reverse engineering of mistakes purposefully creating plausible incorrectness is not only a creative exercise but also a powerful method for building and evaluating more robust correction systems.\n",
            "Similarity Score: 99%\n"
          ]
        }
      ]
    }
  ]
}